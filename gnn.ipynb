{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, global_max_pool\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dir = '../data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atom Featurisation\n",
    "## Auxiliary function for one-hot enconding transformation based on list of\n",
    "##permitted values\n",
    "\n",
    "def one_hot_encoding(x, permitted_list):\n",
    "    \"\"\"\n",
    "    Maps input elements x which are not in the permitted list to the last element\n",
    "    of the permitted list.\n",
    "    \"\"\"\n",
    "    if x not in permitted_list:\n",
    "        x = permitted_list[-1]\n",
    "    binary_encoding = [int(boolean_value) for boolean_value in list(map(lambda s: x == s, permitted_list))]\n",
    "    return binary_encoding\n",
    "    \n",
    "    \n",
    "# Main atom feat. func\n",
    "\n",
    "def get_atom_features(atom, use_chirality=True):\n",
    "    # Define a simplified list of atom types\n",
    "    permitted_atom_types = ['C', 'N', 'O', 'S', 'P', 'F', 'Cl', 'Br', 'I','Dy', 'Unknown']\n",
    "    atom_type = atom.GetSymbol() if atom.GetSymbol() in permitted_atom_types else 'Unknown'\n",
    "    atom_type_enc = one_hot_encoding(atom_type, permitted_atom_types)\n",
    "    \n",
    "    # Consider only the most impactful features: atom degree and whether the atom is in a ring\n",
    "    atom_degree = one_hot_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 'MoreThanFour'])\n",
    "    is_in_ring = [int(atom.IsInRing())]\n",
    "    \n",
    "    #print(atom_degree)\n",
    "    #exit()\n",
    "    # Optionally include chirality\n",
    "    if use_chirality:\n",
    "        chirality_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        atom_features = atom_type_enc + atom_degree + is_in_ring + chirality_enc\n",
    "    else:\n",
    "        atom_features = atom_type_enc + atom_degree + is_in_ring\n",
    "    \n",
    "    return np.array(atom_features, dtype=np.float32)\n",
    "\n",
    "# Bond featurization\n",
    "\n",
    "def get_bond_features(bond):\n",
    "    # Simplified list of bond types\n",
    "    permitted_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC, 'Unknown']\n",
    "    bond_type = bond.GetBondType() if bond.GetBondType() in permitted_bond_types else 'Unknown'\n",
    "    \n",
    "    # Features: Bond type, Is in a ring\n",
    "    features = one_hot_encoding(bond_type, permitted_bond_types) \\\n",
    "               + [int(bond.IsInRing())]\n",
    "    \n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "\n",
    "def create_pytorch_geometric_graph_data_list_from_smiles_and_labels(x_smiles, y=None):\n",
    "    data_list = []\n",
    "    \n",
    "    for index, smiles in enumerate(x_smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        if not mol:  # Skip invalid SMILES strings\n",
    "            continue\n",
    "        \n",
    "        # Node features\n",
    "        atom_features = [get_atom_features(atom) for atom in mol.GetAtoms()]\n",
    "        x = torch.tensor(atom_features, dtype=torch.float)\n",
    "        \n",
    "        # Edge features\n",
    "        edge_index = []\n",
    "        edge_features = []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            edge_index += [(start, end), (end, start)]  # Undirected graph\n",
    "            bond_feature = get_bond_features(bond)\n",
    "            edge_features += [bond_feature, bond_feature]  # Same features in both directions\n",
    "        \n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "        \n",
    "        # Creating the Data object\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        #data.molecule_id = ids[index]\n",
    "        if y is not None:\n",
    "            data.y = torch.tensor([y[index]], dtype=torch.float)\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def create_pytorch_geometric_graph_data_list_from_smiles_and_labels_np(x_smiles, y=None):\n",
    "    data_list = []\n",
    "    \n",
    "    for index, smiles in enumerate(x_smiles):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        if not mol:  # Skip invalid SMILES strings\n",
    "            continue\n",
    "        \n",
    "        # Node features\n",
    "        atom_features = np.array([get_atom_features(atom) for atom in mol.GetAtoms()])\n",
    "        x = torch.tensor(atom_features, dtype=torch.float)\n",
    "        \n",
    "        # Edge features\n",
    "        edge_index = []\n",
    "        edge_features = []\n",
    "        for bond in mol.GetBonds():\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            edge_index += [(start, end), (end, start)]  # Undirected graph\n",
    "            bond_feature = get_bond_features(bond)\n",
    "            edge_features += [bond_feature, bond_feature]  # Same features in both directions\n",
    "        \n",
    "        edge_index = torch.tensor(np.array(edge_index), dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(np.array(edge_features), dtype=torch.float)\n",
    "        \n",
    "        # Creating the Data object\n",
    "        #data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        #data.molecule_id = ids[index]\n",
    "        data = [x,edge_index,edge_attr]\n",
    "        if y is not None:\n",
    "            data.y = torch.tensor(np.array([y[index]]), dtype=torch.float)\n",
    "        \n",
    "        data_list.append(data)\n",
    "    \n",
    "    return data_list\n",
    "\n",
    "def featurize_data_in_batches(smiles_list, labels_list, batch_size):\n",
    "    data_list = []\n",
    "    # Define tqdm progress bar\n",
    "    pbar = tqdm(total=len(smiles_list), desc=\"Featurizing data\")\n",
    "    for i in range(0, len(smiles_list), batch_size):\n",
    "        smiles_batch = smiles_list[i:i+batch_size]\n",
    "        if labels_list is not None:\n",
    "            labels_batch = labels_list[i:i+batch_size]\n",
    "        else:\n",
    "            labels_batch = None\n",
    "        #ids_batch = ids_list[i:i+batch_size]\n",
    "        #batch_data_list = create_pytorch_geometric_graph_data_list_from_smiles_and_labels(smiles_batch, labels_batch)\n",
    "        batch_data_list = create_pytorch_geometric_graph_data_list_from_smiles_and_labels_np(smiles_batch, labels_batch)\n",
    "\n",
    "        data_list.extend(batch_data_list)\n",
    "        pbar.update(len(smiles_batch))\n",
    "        \n",
    "    pbar.close()\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m dtypes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuildingblock1_smiles\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuildingblock2_smiles\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuildingblock3_smiles\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[1;32m      2\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds_BRD4\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39mbyte, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds_HSA\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39mbyte, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinds_sEH\u001b[39m\u001b[38;5;124m'\u001b[39m:np\u001b[38;5;241m.\u001b[39mbyte}\n\u001b[0;32m----> 4\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../shrunken_data/train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train))\n\u001b[1;32m      6\u001b[0m train\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/data/aa519/installations/anaconda3/envs/birdkaggle/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/aa519/installations/anaconda3/envs/birdkaggle/lib/python3.9/site-packages/pandas/io/parsers/readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/aa519/installations/anaconda3/envs/birdkaggle/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m     (\n\u001b[1;32m   1920\u001b[0m         index,\n\u001b[1;32m   1921\u001b[0m         columns,\n\u001b[1;32m   1922\u001b[0m         col_dict,\n\u001b[0;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/data/aa519/installations/anaconda3/envs/birdkaggle/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "dtypes = {'buildingblock1_smiles': np.int16, 'buildingblock2_smiles': np.int16, 'buildingblock3_smiles': np.int16,\n",
    "          'binds_BRD4':np.byte, 'binds_HSA':np.byte, 'binds_sEH':np.byte}\n",
    "\n",
    "train = pd.read_csv('../shrunken_data/train.csv', dtype = dtypes)\n",
    "print(len(train))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['binds_BRD4','binds_HSA','binds_sEH']].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing data: 100%|██████████| 3000/3000 [00:03<00:00, 968.95it/s] \n"
     ]
    }
   ],
   "source": [
    "# Define the batch size for featurization\n",
    "batch_size = 2**8\n",
    "smiles_list = df['molecule_smiles'].tolist()\n",
    "labels_list = df[['binds_BRD4','binds_HSA','binds_sEH']].values\n",
    "train_data = featurize_data_in_batches(smiles_list, labels_list, batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878022\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buildingblock1_smiles</th>\n",
       "      <th>buildingblock2_smiles</th>\n",
       "      <th>buildingblock3_smiles</th>\n",
       "      <th>molecule_smiles</th>\n",
       "      <th>is_BRD4</th>\n",
       "      <th>is_HSA</th>\n",
       "      <th>is_sEH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>87</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>99</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(NCC2(O)CCCC2(C)C)nc(Nc2ccc(C...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>244</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2sc(Cl)c...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>394</td>\n",
       "      <td>C#CCCC[C@H](Nc1nc(NCC2CCC(SC)CC2)nc(Nc2ccc(C=C...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   buildingblock1_smiles  buildingblock2_smiles  buildingblock3_smiles  \\\n",
       "0                      0                     17                     17   \n",
       "1                      0                     17                     87   \n",
       "2                      0                     17                     99   \n",
       "3                      0                     17                    244   \n",
       "4                      0                     17                    394   \n",
       "\n",
       "                                     molecule_smiles  is_BRD4  is_HSA  is_sEH  \n",
       "0  C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ccc(C=C...     True    True    True  \n",
       "1  C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2ncnc3c2...     True    True    True  \n",
       "2  C#CCCC[C@H](Nc1nc(NCC2(O)CCCC2(C)C)nc(Nc2ccc(C...     True    True    True  \n",
       "3  C#CCCC[C@H](Nc1nc(Nc2ccc(C=C)cc2)nc(Nc2sc(Cl)c...     True    True    True  \n",
       "4  C#CCCC[C@H](Nc1nc(NCC2CCC(SC)CC2)nc(Nc2ccc(C=C...     True    True    True  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes = {'buildingblock1_smiles': np.int16, 'buildingblock2_smiles': np.int16, 'buildingblock3_smiles': np.int16,\n",
    "          'binds_BRD4':np.byte, 'binds_HSA':np.byte, 'binds_sEH':np.byte}\n",
    "\n",
    "test = pd.read_csv('../shrunken_data/test.csv', dtype = dtypes)\n",
    "print(len(test))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_simple = test[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Featurizing data: 100%|██████████| 100000/100000 [01:40<00:00, 990.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 40s, sys: 179 ms, total: 1min 41s\n",
      "Wall time: 1min 40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define the batch size for featurization\n",
    "batch_size = 64\n",
    "smiles_list = test_simple['molecule_smiles'].tolist()\n",
    "#labels_list = test[['binds_BRD4','binds_HSA','binds_sEH']].values\n",
    "test_data = featurize_data_in_batches(smiles_list, None, batch_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELLING\n",
    "\n",
    "#Define custom GNN layer\n",
    "class CustomGNNLayer(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(CustomGNNLayer, self).__init__(aggr='max')\n",
    "        self.lin = nn.Linear(in_channels + 6, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Start propagating messages\n",
    "        return MessagePassing.propagate(self, edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        combined = torch.cat((x_j, edge_attr), dim=1)\n",
    "        return combined\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return self.lin(aggr_out)\n",
    "\n",
    "#Define GNN Model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout_rate,out_channels=1):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList([CustomGNNLayer(input_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.bns = nn.ModuleList([nn.BatchNorm1d(hidden_dim) for _ in range(num_layers)])\n",
    "        self.lin = nn.Linear(hidden_dim, out_channels)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.dropout(x)\n",
    "\n",
    "\n",
    "        x = global_max_pool(x, data.batch) # Global pooling to get a graph-level representation\n",
    "        x = self.lin(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader, num_epochs, input_dim, hidden_dim, num_layers, dropout_rate,out_channels, lr):\n",
    "    model = GNNModel(input_dim, hidden_dim, num_layers, dropout_rate,out_channels)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch)\n",
    "            #loss = criterion(out, batch.y.view(-1, 1).float()) # ??\n",
    "            loss = criterion(out, batch.y.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader)}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def predict_with_model(model, test_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    #molecule_ids = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            output = torch.sigmoid(model(data))\n",
    "            predictions.extend(output.view(-1).tolist())\n",
    "            #molecule_ids.extend(data.molecule_id)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def select_and_save_predictions_with_ids(predictions,test_df,path_test_file_for_ids,output_dir = 'results/',output_file_name ='ids_pred_results.csv'):\n",
    "    \n",
    "    #Combine predictions with the bools\n",
    "    bool_cols  = test_df[['is_BRD4','is_HSA','is_sEH']]\n",
    "    bool_cols = np.array(bool_cols).reshape(-1)\n",
    "    y_pred_and_bools =  np.vstack((bool_cols,predictions)).T\n",
    "    y_pred_and_bools_df = pd.DataFrame({'Bool': y_pred_and_bools[:, 0], 'binds': y_pred_and_bools[:, 1]})\n",
    "    \n",
    "    # drop predictions of protiens not in the test set and also drop the bool column\n",
    "    y_pred_and_bools_df = y_pred_and_bools_df[y_pred_and_bools_df.Bool != 0]\n",
    "    y_pred_df = y_pred_and_bools_df.drop(['Bool'],axis = 1)\n",
    "    y_pred_df = y_pred_df.reset_index(drop=True)\n",
    "\n",
    "    #read the test ids\n",
    "    test = pd.read_csv(path_test_file_for_ids,index_col=False)[:len(y_pred_df)]\n",
    "    test_ids = pd.DataFrame(test.id)\n",
    "    assert len(y_pred_df)==len(test_ids)\n",
    "    y_pred_and_ids_df = pd.concat([test_ids,y_pred_df],axis=1)\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir,output_file_name)\n",
    "    y_pred_and_ids_df.to_csv(output_path,index=False)\n",
    "    return y_pred_and_ids_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11, Loss: 0.053738421076869075\n",
      "Epoch 2/11, Loss: 0.008369789972772544\n",
      "Epoch 3/11, Loss: 0.00835864261372332\n",
      "Epoch 4/11, Loss: 0.008439316725769535\n",
      "Epoch 5/11, Loss: 0.007954135739016644\n",
      "Epoch 6/11, Loss: 0.008297347993728645\n",
      "Epoch 7/11, Loss: 0.007923215554601097\n",
      "Epoch 8/11, Loss: 0.007673561386020973\n",
      "Epoch 9/11, Loss: 0.007450859184362034\n",
      "Epoch 10/11, Loss: 0.007932277248325223\n",
      "Epoch 11/11, Loss: 0.008421062985980051\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "input_dim = train_loader.dataset[0].num_node_features\n",
    "hidden_dim = 64\n",
    "num_epochs = 11\n",
    "num_layers = 4 #Should ideally be set so that all nodes can communicate with each other\n",
    "dropout_rate = 0.3\n",
    "lr = 0.001\n",
    "out_channels =3\n",
    "#These are just example values, feel free to play around with them.\n",
    "model = train_model(train_loader,num_epochs, input_dim, hidden_dim,num_layers, dropout_rate,out_channels, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "predictions = predict_with_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_file_for_ids = '../data/test.csv'\n",
    "output_df = select_and_save_predictions_with_ids(predictions,test,path_test_file_for_ids,output_dir = 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>binds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>295246830</td>\n",
       "      <td>0.125133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>295246831</td>\n",
       "      <td>0.133689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>295246832</td>\n",
       "      <td>0.099996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>295246833</td>\n",
       "      <td>0.124910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295246834</td>\n",
       "      <td>0.144258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>295258825</td>\n",
       "      <td>0.114499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>295258826</td>\n",
       "      <td>0.115671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>295258827</td>\n",
       "      <td>0.120788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>295258828</td>\n",
       "      <td>0.115426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>295258829</td>\n",
       "      <td>0.114354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id     binds\n",
       "0      295246830  0.125133\n",
       "1      295246831  0.133689\n",
       "2      295246832  0.099996\n",
       "3      295246833  0.124910\n",
       "4      295246834  0.144258\n",
       "...          ...       ...\n",
       "11995  295258825  0.114499\n",
       "11996  295258826  0.115671\n",
       "11997  295258827  0.120788\n",
       "11998  295258828  0.115426\n",
       "11999  295258829  0.114354\n",
       "\n",
       "[12000 rows x 2 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "birdkaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
